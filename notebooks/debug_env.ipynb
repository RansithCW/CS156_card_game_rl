{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7ae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cfa424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per game: -7.892434210526317\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Testing if model runs\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from env.game_engine.rl_engine import index_to_card\n",
    "from env.tnf_env import TNFEnv\n",
    "from agents.rl_agent import RLAgent\n",
    "\n",
    "model_ppo = PPO.load(\"../models/stage_mixed/tnf_mixed_150000steps\")\n",
    "rlagent = RLAgent(model_ppo)\n",
    "env = TNFEnv(verbose=True)\n",
    "\n",
    "n = 100 # no. of games/episodes to run\n",
    "\n",
    "tot_reward = 0\n",
    "\n",
    "for episode in range(n):\n",
    "    obs, info = env.reset()\n",
    "    # print(\"trumps:\", env.game.trump_suit)\n",
    "    # print(\"Agent hand:\", [str(i) for i in env.game.hands[env.agent_id]])\n",
    "    curr_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = rlagent.select_action(obs, info[\"action_mask\"])\n",
    "        obs, r, done, _, info = env.step(int(action))\n",
    "        curr_reward += r\n",
    "        table = info.get(\"resolved_tricks\")\n",
    "    tot_reward += curr_reward\n",
    "    # print(f\"Episode {episode} finished with {tot_reward} value for rl_agent's team.\")\n",
    "    \n",
    "    \n",
    "print(\"Average reward per game:\", tot_reward/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de09e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0b9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a2f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9954ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
