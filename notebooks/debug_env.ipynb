{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7ae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cfa424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per game: 23.64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Testing if model runs\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from env.game_engine.rl_engine import index_to_card\n",
    "from env.tnf_env import TNFEnv\n",
    "from agents.rl_agent import RLAgent\n",
    "\n",
    "model_ppo = MaskablePPO.load(\"./models/snapshots_mixed/snapshot_50000\")\n",
    "rlagent = RLAgent(model_ppo)\n",
    "env = TNFEnv(verbose=False)\n",
    "\n",
    "n = 100 # no. of games/episodes to run\n",
    "\n",
    "tot_reward = 0\n",
    "\n",
    "for episode in range(n):\n",
    "    obs, info = env.reset()\n",
    "    # print(\"trumps:\", env.game.trump_suit)\n",
    "    # print(\"Agent hand:\", [str(i) for i in env.game.hands[env.agent_id]])\n",
    "    curr_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = rlagent.select_action(obs, info.get(\"action_mask\"))\n",
    "        obs, r, done, _, info = env.step(int(action))\n",
    "        curr_reward += r\n",
    "        table = info.get(\"resolved_tricks\")\n",
    "    tot_reward += curr_reward\n",
    "    # print(f\"Episode {episode} finished with {tot_reward} value for rl_agent's team.\")\n",
    "    \n",
    "    \n",
    "print(\"Average reward per game:\", tot_reward/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83de09e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating NEW MaskablePPO model for mixed training ---\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'log_dir' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraining\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_ppo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Set device to 'cuda' for Colab's GPU\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgame_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmixed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\training\\train_ppo.py:60\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(game_type, total_timesteps, load_path, device, check_freq, logging)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- Creating NEW MaskablePPO model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m training ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m     model = MaskablePPO(\n\u001b[32m     55\u001b[39m         policy=\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m         env=env,\n\u001b[32m     57\u001b[39m         verbose=\u001b[32m1\u001b[39m,\n\u001b[32m     58\u001b[39m         ent_coef=\u001b[32m0.01\u001b[39m, \u001b[38;5;66;03m# Keep exploration high during early stages\u001b[39;00m\n\u001b[32m     59\u001b[39m         learning_rate=\u001b[32m3e-4\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         tensorboard_log=\u001b[43mlog_dir\u001b[49m,\n\u001b[32m     61\u001b[39m         device=device\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# 3. Setup Self-Play Callback\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# check_freq: how often to update opponents (e.g., every 50,000 steps)\u001b[39;00m\n\u001b[32m     66\u001b[39m self_play_callback = SelfPlayCallback(\n\u001b[32m     67\u001b[39m     check_freq=check_freq, \n\u001b[32m     68\u001b[39m     save_path=snapshot_dir\n\u001b[32m     69\u001b[39m )\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'log_dir' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from training.train_ppo import train\n",
    "\n",
    "# Set device to 'cuda' for Colab's GPU\n",
    "train(\n",
    "    game_type='mixed', \n",
    "    total_timesteps=500_000, \n",
    "    device='cuda',\n",
    "    check_freq=20_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0b9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a2f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9954ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
