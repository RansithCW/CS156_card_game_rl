{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21cfa424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per game: 6.598039473684209\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Testing if model runs\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from env.game_engine.rl_engine import index_to_card\n",
    "from env.tnf_env import TNFEnv\n",
    "from agents.rl_agent import RLAgent\n",
    "\n",
    "model_ppo = MaskablePPO.load(\"../models/stage_greedy/tnf_greedy_300000_steps\")\n",
    "rlagent = RLAgent(model_ppo)\n",
    "env = TNFEnv(verbose=True)\n",
    "\n",
    "n = 1000 # no. of games/episodes to run\n",
    "\n",
    "tot_reward = 0\n",
    "\n",
    "for episode in range(n):\n",
    "    obs, info = env.reset()\n",
    "    # print(\"trumps:\", env.game.trump_suit)\n",
    "    # print(\"Agent hand:\", [str(i) for i in env.game.hands[env.agent_id]])\n",
    "    curr_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = rlagent.select_action(obs, info[\"action_mask\"])\n",
    "        obs, r, done, _, info = env.step(int(action))\n",
    "        curr_reward += r\n",
    "        table = info.get(\"resolved_tricks\")\n",
    "    tot_reward += curr_reward\n",
    "    # print(f\"Episode {episode} finished with {tot_reward} value for rl_agent's team.\")\n",
    "    \n",
    "    \n",
    "print(\"Average reward per game:\", tot_reward/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83de09e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating NEW MaskablePPO model for mixed training ---\n",
      "Using cuda device\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Trying to log data to tensorboard but tensorboard is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraining\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_ppo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Set device to 'cuda' for Colab's GPU\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgame_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmixed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\training\\train_ppo.py:71\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(game_type, total_timesteps, load_path, device)\u001b[39m\n\u001b[32m     65\u001b[39m self_play_callback = SelfPlayCallback(\n\u001b[32m     66\u001b[39m     check_freq=\u001b[32m50000\u001b[39m, \n\u001b[32m     67\u001b[39m     save_path=snapshot_dir\n\u001b[32m     68\u001b[39m )\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# 4. Train with Callback\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_steps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_play_callback\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <--- IMPORTANT\u001b[39;49;00m\n\u001b[32m     76\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 5. SAVE final model and normalization stats\u001b[39;00m\n\u001b[32m     79\u001b[39m save_dir = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodels/stage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\sb3_contrib\\ppo_mask\\ppo_mask.py:441\u001b[39m, in \u001b[36mMaskablePPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfMaskablePPO,\n\u001b[32m    431\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    437\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    438\u001b[39m ) -> SelfMaskablePPO:\n\u001b[32m    439\u001b[39m     iteration = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     total_timesteps, callback = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     callback.on_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:431\u001b[39m, in \u001b[36mBaseAlgorithm._setup_learn\u001b[39m\u001b[34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Configure logger's outputs if no logger was passed\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._custom_logger:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger = \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigure_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[32m    434\u001b[39m callback = \u001b[38;5;28mself\u001b[39m._init_callback(callback, progress_bar)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:287\u001b[39m, in \u001b[36mconfigure_logger\u001b[39m\u001b[34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[39m\n\u001b[32m    284\u001b[39m save_path, format_strings = \u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[33m\"\u001b[39m\u001b[33mstdout\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTrying to log data to tensorboard but tensorboard is not installed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    290\u001b[39m     latest_run_id = get_latest_run_id(tensorboard_log, tb_log_name)\n",
      "\u001b[31mImportError\u001b[39m: Trying to log data to tensorboard but tensorboard is not installed."
     ]
    }
   ],
   "source": [
    "from training.train_ppo import train\n",
    "\n",
    "# Set device to 'cuda' for Colab's GPU\n",
    "train(\n",
    "    game_type='mixed', \n",
    "    total_timesteps=500_000, \n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0b9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a2f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9954ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
