{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7ae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cfa424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x145 and 110x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     action = \u001b[43mrlagent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maction_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     obs, r, done, _, info = env.step(\u001b[38;5;28mint\u001b[39m(action))\n\u001b[32m     27\u001b[39m     curr_reward += r\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\agents\\rl_agent.py:29\u001b[39m, in \u001b[36mRLAgent.select_action\u001b[39m\u001b[34m(self, obs, action_mask)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     28\u001b[39m     obs_tensor = torch.as_tensor(obs).to(\u001b[38;5;28mself\u001b[39m.model.device)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     dist = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     logits = dist.distribution.logits.clone() \u001b[38;5;66;03m# shape [1, 32]\u001b[39;00m\n\u001b[32m     33\u001b[39m     mask = torch.as_tensor(action_mask).to(logits.device) \u001b[38;5;66;03m#[32]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:751\u001b[39m, in \u001b[36mActorCriticPolicy.get_distribution\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[33;03mGet the current policy distribution given the observations.\u001b[39;00m\n\u001b[32m    746\u001b[39m \n\u001b[32m    747\u001b[39m \u001b[33;03m:param obs:\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[33;03m:return: the action distribution.\u001b[39;00m\n\u001b[32m    749\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    750\u001b[39m features = \u001b[38;5;28msuper\u001b[39m().extract_features(obs, \u001b[38;5;28mself\u001b[39m.pi_features_extractor)\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m latent_pi = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_action_dist_from_latent(latent_pi)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:260\u001b[39m, in \u001b[36mMlpExtractor.forward_actor\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th.Tensor) -> th.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1x145 and 110x64)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Testing if model runs\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from env.game_engine.rl_engine import index_to_card\n",
    "from env.tnf_env import TNFEnv\n",
    "from agents.rl_agent import RLAgent\n",
    "\n",
    "model_ppo = PPO.load(\"../models/tnf_ppo\")\n",
    "rlagent = RLAgent(model_ppo)\n",
    "env = TNFEnv(verbose=True)\n",
    "\n",
    "n = 1000 # no. of games/episodes to run\n",
    "\n",
    "tot_reward = 0\n",
    "\n",
    "for episode in range(n):\n",
    "    obs, info = env.reset()\n",
    "    # print(\"trumps:\", env.game.trump_suit)\n",
    "    # print(\"Agent hand:\", [str(i) for i in env.game.hands[env.agent_id]])\n",
    "    curr_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = rlagent.select_action(obs, info[\"action_mask\"])\n",
    "        obs, r, done, _, info = env.step(int(action))\n",
    "        curr_reward += r\n",
    "        table = info.get(\"resolved_tricks\")\n",
    "    tot_reward += curr_reward\n",
    "    # print(f\"Episode {episode} finished with {tot_reward} value for rl_agent's team.\")\n",
    "    \n",
    "    \n",
    "print(\"Average reward per game:\", tot_reward/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de09e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0b9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a2f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9954ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
