{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7ae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "model_path = r\"C:\\Users\\ransi\\Downloads\\snapshot_50000.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfa424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # Testing if model runs\n",
    "\n",
    "# from sb3_contrib import MaskablePPO\n",
    "# from env.game_engine.rl_engine import index_to_card\n",
    "# from env.tnf_env import TNFEnv\n",
    "# from agents.rl_agent import RLAgent\n",
    "\n",
    "\n",
    "# model_ppo = MaskablePPO.load(model_path)\n",
    "# rlagent = RLAgent(model_ppo)\n",
    "# env = TNFEnv(verbose=False)\n",
    "\n",
    "# n = 5 # no. of games/episodes to run\n",
    "\n",
    "# tot_reward = 0\n",
    "\n",
    "# for episode in range(n):\n",
    "#     obs, info = env.reset()\n",
    "#     # print(\"trumps:\", env.game.trump_suit)\n",
    "#     # print(\"Agent hand:\", [str(i) for i in env.game.hands[env.agent_id]])\n",
    "#     curr_reward = 0\n",
    "#     done = False\n",
    "\n",
    "#     while not done:\n",
    "#         action = rlagent.select_action(obs, info.get(\"action_mask\"))\n",
    "#         obs, r, done, _, info = env.step(int(action))\n",
    "#         curr_reward += r\n",
    "#         table = info.get(\"resolved_tricks\")\n",
    "#     tot_reward += curr_reward\n",
    "#     # print(f\"Episode {episode} finished with {tot_reward} value for rl_agent's team.\")\n",
    "    \n",
    "    \n",
    "# print(\"Average reward per game:\", tot_reward/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from training.train_ppo import train\n",
    "\n",
    "# # Set device to 'cuda' for Colab's GPU\n",
    "# train(\n",
    "#     game_type='mixed', \n",
    "#     total_timesteps=500_000, \n",
    "#     device='cuda',\n",
    "#     check_freq=20_000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a0b9eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meval_vs_baseline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\evaluation\\eval_vs_baseline.py:45\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(num_episodes, render, model_path)\u001b[39m\n\u001b[32m     43\u001b[39m rewards = []\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     rewards.append(\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean reward over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m episodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStd: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.std(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\evaluation\\eval_vs_baseline.py:25\u001b[39m, in \u001b[36mrun_episode\u001b[39m\u001b[34m(env, agents, render)\u001b[39m\n\u001b[32m     22\u001b[39m     total_reward += reward\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_reward\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ransi\\OneDrive - minerva.edu\\Desktop\\Random Projects\\three_nought_four_rl\\rl_env\\Lib\\site-packages\\gymnasium\\core.py:191\u001b[39m, in \u001b[36mEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    160\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute the render frames as specified by :attr:`render_mode` during the initialization of the environment.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03m    The environment's :attr:`metadata` render modes (`env.metadata[\"render_modes\"]`) should contain the possible\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m \u001b[33;03m        in the environment initialised, i.e., ``gymnasium.make(\"CartPole-v1\", render_mode=\"human\")``\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from evaluation.eval_vs_baseline import evaluate\n",
    "\n",
    "evaluate(500, True, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a2f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9954ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f312fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
